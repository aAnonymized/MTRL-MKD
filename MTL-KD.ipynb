{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43265239-e20e-45e9-8742-05f6ba404ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/chenweilin/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../CMR-AI/mmaction/')\n",
    "sys.path.append('../CMR-AI/')\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from swinTransformer3D_origin import SwinTransformer3D\n",
    "from models.losses.mutil_class_loss import FocalLoss, cal_auc\n",
    "from models.losses.weighted_auc_f1 import get_weighted_auc_f1\n",
    "from load_dataset import ACDC\n",
    "from utilsss import generate_mask_matrix, pruning_mask, row_softmax\n",
    "from Policy import Policy, train_agent\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e2dcfa-ca3a-48f3-b248-3ef55cf05a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3214.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model is 3\n"
     ]
    }
   ],
   "source": [
    "num_class = {\n",
    "        0: ['HCM', 'RV', 'DCM', 'MINF'],\n",
    "        1: ['HCM', 'RV'], \n",
    "        2: ['DCM', 'MINF'],\n",
    "    }\n",
    "\n",
    "save_models = {0: 'full', 1: '1', 2: '2'}\n",
    "dummy_labels = num_class[0] # taken from paper\n",
    "\n",
    "models = []\n",
    "for _, v in num_class.items():\n",
    "    num_class_ = len(v)\n",
    "    models.append(SwinTransformer3D(num_class=num_class_))\n",
    "print(f'Total model is {len(models)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9dc1377-6eab-4e27-8686-aae81fef7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "层名: head.fc_cls.weight | 大小: torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "input_size, teacher_num = 0, len(models)\n",
    "for name, param in list(models[0].named_parameters())[-2:-1]:\n",
    "    print(f\"层名: {name} | 大小: {param.size()}\")\n",
    "    input_size = param.size()[-1]\n",
    "agent = Policy(input_size=input_size, teacher_num=teacher_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f243e565-2372-4d11-98da-9a5e0a554e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weightd from local file pth.\n",
      "No.1 model load pretrained weighted end.\n",
      "load weightd from local file pth.\n",
      "No.2 model load pretrained weighted end.\n",
      "load weightd from local file pth.\n",
      "No.3 model load pretrained weighted end.\n"
     ]
    }
   ],
   "source": [
    "load_swin_base_patch244_window877_kinetics600_22k = False\n",
    "for i in range(len(models)):\n",
    "    if load_swin_base_patch244_window877_kinetics600_22k:\n",
    "        _pretrained_dict = torch.load(f'./ACDC/MTRL-inner-NOKL/{save_models[i]}/epoch_200.pth') \n",
    "        pretrained_dict = {}\n",
    "        for k, v in _pretrained_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                new_key = k.replace('module.', '')\n",
    "            # 移除 'cls_head.' 前缀\n",
    "            elif k.startswith('cls_head.'):\n",
    "                new_key = k.replace('cls_head.', '')\n",
    "            else:\n",
    "                new_key = k  # 如果没有前缀，则保持不变\n",
    "\n",
    "            pretrained_dict[new_key] = v\n",
    "    else:\n",
    "        print(f'load weightd from local file pth.')\n",
    "        _pretrained_dict = torch.load(f'./ACDC/MTL-KD/epoch_75.pth')\n",
    "        pretrained_dict = {}\n",
    "        for k, v in _pretrained_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                new_key = k.replace('module.', '')\n",
    "            # 移除 'cls_head.' 前缀\n",
    "            elif k.startswith('cls_head.'):\n",
    "                new_key = k.replace('cls_head.', '')\n",
    "            else:\n",
    "                new_key = k  # 如果没有前缀，则保持不变\n",
    "\n",
    "            pretrained_dict[new_key] = v\n",
    "    model_dict = models[i].state_dict()\n",
    "    # 过滤掉不匹配的参数\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    models[i].load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f'No.{i+1} model load pretrained weighted end.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d021bd5-6941-42e9-8874-3fe15c84efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./roi_processed/training/train_data.csv', encoding='GBK')\n",
    "test_data = pd.read_csv('./roi_processed/testing/test_data.csv', encoding='GBK')\n",
    "dummy_labels = ['HCM', 'RV', 'DCM', 'MINF']  # ['HCM', 'DCM', 'MINF', 'RV', 'NOR'] # taken from paper\n",
    "train_data = train_data[train_data['Finding Labels'].isin(dummy_labels)]\n",
    "test_data = test_data[test_data['Finding Labels'].isin(dummy_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c29be464-2ea9-4bcf-8094-d22ca218dfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    train_data[label] = train_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9357285b-ecf1-40ad-9ef5-7f7f4d490bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    test_data[label] = test_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600a4135-e785-4364-b605-c2515eb40907",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target_vector'] = train_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])\n",
    "\n",
    "test_data['target_vector'] = test_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4ecae2-6ef9-4e38-ba10-a785847dd3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集：\n",
      "HCM     20.0\n",
      "RV      20.0\n",
      "DCM     20.0\n",
      "MINF    20.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clean_labels = train_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(f'训练数据集：')\n",
    "print(clean_labels) # view tabular results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b0d773-93fc-4ca7-8cf9-6f6194900603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据集：\n",
      "HCM     10.0\n",
      "RV      10.0\n",
      "DCM     10.0\n",
      "MINF    10.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'测试数据集：')\n",
    "clean_labels = test_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(clean_labels) # view tabular results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a00694-9097-40a2-8de9-0cda171b89c7",
   "metadata": {},
   "source": [
    "## 训练开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a420aaea-5690-4458-b7cf-232de331180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.0005\n",
    "batch_size = 8\n",
    "max_epoch = 600\n",
    "\n",
    "phase = 'train'\n",
    "  \n",
    "for i in range(len(models)):\n",
    "    models[i] = models[i].cuda()\n",
    "    \n",
    "fn_loss  = FocalLoss(device = 'cuda:0', gamma = 2.).to('cuda:0')\n",
    "kl_loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "cross_loss = nn.CrossEntropyLoss()\n",
    "optimizers = []\n",
    "for i in range(len(models)):\n",
    "    optimizers.append(torch.optim.SGD(models[i].parameters(), lr=base_lr))\n",
    "    \n",
    "agent_optimizer = torch.optim.SGD(agent.parameters(), lr=base_lr)\n",
    "    \n",
    "train_acdc_data = ACDC(data=train_data, phase = 'train', img_size=(224, 224))\n",
    "train_data_loader = DataLoader(train_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "test_acdc_data = ACDC(data=test_data, phase = 'test', img_size=(224, 224))\n",
    "test_data_loader = DataLoader(test_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac16c78-cf32-4804-9e16-aa638f320ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# 初始化 TensorBoard\n",
    "writer = SummaryWriter(log_dir='./runs/VSTMTRL-ACDC')  # 指定日志保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14b1ca-a396-4cb9-ab02-ccedcc23ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == 'train':\n",
    "    import time\n",
    "    pre_mutil_teacher_acc = []\n",
    "    now_mutil_teacher_acc = []\n",
    "    for i in range(1, len(models)):\n",
    "        pre_mutil_teacher_acc.append(0)\n",
    "        now_mutil_teacher_acc.append(0)\n",
    "    logit_actions_list = []\n",
    "    agent_rewars_list  = []\n",
    "    student_infos_list = []    \n",
    "    for epoch_num in range(0, max_epoch):\n",
    "        print(f\"--------> epoch_num: {epoch_num}\")\n",
    "        train_loader_nums = len(train_data_loader.dataset)\n",
    "        probs = np.zeros((train_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "        gt    = np.zeros((train_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "        k_index = 0\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        for i in range(len(models)): models[i].train()\n",
    "        mutil_teacher_correct = []\n",
    "        mutil_teacher_num = []\n",
    "        for i in range(1, len(models)):\n",
    "            mutil_teacher_correct.append(0)\n",
    "            mutil_teacher_num.append(0)\n",
    "        ## 全局定义\n",
    "        # s_train_loss = torch.tensor(0.0, device='cuda')  # 创建在GPU上的零张量\n",
    "        for batch_idx, (batch_data, batch_finding, batch_label) in enumerate(train_data_loader):\n",
    "            weighted_list = []\n",
    "            mutil_teacher_label = torch.zeros_like(batch_label)\n",
    "            pre_lables = 0\n",
    "            ## 局部赋值\n",
    "            s_train_loss = 0.01\n",
    "            for i, k in enumerate(num_class.keys()):\n",
    "                if i == 0: \n",
    "                    continue\n",
    "                else:\n",
    "                    ## train teacher.\n",
    "                    teacher_train_data_index = pd.Series(batch_finding).isin(num_class[k])\n",
    "                    teacher_train_data_index = teacher_train_data_index.to_numpy()\n",
    "                    weighted_list.append(np.sum(teacher_train_data_index > 0))\n",
    "                    mutil_teacher_num[i-1] += weighted_list[i-1]\n",
    "                    t_train_data = batch_data[teacher_train_data_index]\n",
    "                    t_train_label = batch_label[teacher_train_data_index][:, pre_lables:pre_lables+len(num_class[k])]\n",
    "                    if np.sum(teacher_train_data_index > 0) == 0: \n",
    "                        pre_lables += len(num_class[i])\n",
    "                        continue\n",
    "                    t_output, _ = models[i](t_train_data.cuda())\n",
    "                    mutil_teacher_label[teacher_train_data_index, pre_lables:pre_lables+len(num_class[k])] = \\\n",
    "                                            row_softmax(t_output.cpu().detach())\n",
    "                    pre_lables += len(num_class[k])\n",
    "                    t_output = t_output.reshape(t_output.shape[0], -1)\n",
    "                    t_train_label = t_train_label.reshape(t_train_label.shape[0], -1).cuda()\n",
    "                    t_train_loss = fn_loss(t_output, t_train_label)\n",
    "                    optimizers[i].zero_grad()\n",
    "                    t_train_loss.backward()\n",
    "                    optimizers[i].step()\n",
    "                    predicted_ = torch.argmax(t_output, 1)\n",
    "                    labels_ = torch.argmax(t_train_label.cuda(), 1)\n",
    "                    correct_ = (predicted_ == labels_).sum().item() \n",
    "                    mutil_teacher_correct[i-1] += correct_\n",
    "            \n",
    "            outputs, student_info = models[0](batch_data.cuda())\n",
    "            outputs = outputs.reshape(outputs.shape[0], -1)\n",
    "            \n",
    "            mutil_teacher_label = mutil_teacher_label.reshape(mutil_teacher_label.shape[0], -1).cuda()\n",
    "            probs[k_index: k_index + outputs.shape[0], :] = outputs.cpu().detach().numpy()\n",
    "            gt[   k_index: k_index + outputs.shape[0], :] = batch_label.cpu().detach().numpy()\n",
    "            k_index += outputs.shape[0]\n",
    "            \n",
    "            if (epoch_num+1) <= 100:\n",
    "                log_outputs = torch.nn.LogSoftmax(dim=1)(outputs)            \n",
    "                train_loss = cross_loss(outputs, batch_label.cuda()) + 0.5*kl_loss(log_outputs, mutil_teacher_label.cuda())\n",
    "            else:\n",
    "                 train_loss = cross_loss(outputs, batch_label.cuda())\n",
    "                    \n",
    "            optimizers[0].zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizers[0].step()\n",
    "            total_train_loss += train_loss\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            labels = torch.argmax(batch_label.cuda(), 1)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "                \n",
    "        print(f\"epoch_num {epoch_num} train loss {total_train_loss} \")  \n",
    "        \n",
    "        writer.add_scalars('Training Metrics', {\n",
    "            'Loss': total_train_loss,\n",
    "            'Accuracy': correct / train_loader_nums,\n",
    "        }, epoch_num)\n",
    "        \n",
    "        lr_ = base_lr*(1-0.0009)\n",
    "        for i in range(len(optimizers)):\n",
    "            for param_group in optimizers[i].param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"程序运行时间：{elapsed_time} 秒\")\n",
    "        \n",
    "        save_interval = 25\n",
    "        if (epoch_num + 1) % save_interval == 0:\n",
    "            for i in range(len(models)):\n",
    "                save_mode_path = os.path.join(f'./ACDC/MTL-KD', 'epoch_' + str(epoch_num+1) + '.pth')\n",
    "                torch.save(models[i].state_dict(), save_mode_path)\n",
    "                print(\"save model to {}\".format(save_mode_path))\n",
    "                \n",
    "        test_interval = 25\n",
    "        if (epoch_num + 1) % test_interval == 0:\n",
    "            ### 模型验证\n",
    "            test_loader_nums = len(test_data_loader.dataset)\n",
    "            test_probs = np.zeros((test_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "            test_gt    = np.zeros((test_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "            test_k  =0\n",
    "            models[0].eval()\n",
    "            with torch.no_grad():\n",
    "                for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "                    test_data_batch = test_data_batch.cuda()\n",
    "                    test_label_batch = test_label_batch.cuda()\n",
    "                    test_outputs, _ = models[0](test_data_batch.cuda())\n",
    "                    test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "                    test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "                    test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "                    test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "                    test_k += test_outputs.shape[0]\n",
    "                test_label = np.argmax(test_gt, axis=1)\n",
    "                test_pred = np.argmax(test_probs, axis=1)\n",
    "                get_weighted_auc_f1(test_probs, test_pred, test_label)\n",
    "                print(f\"acc: {np.sum(test_label==test_pred)/test_loader_nums}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e417e409-e953-4cf6-932e-31ced82c65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "auc_list : [0.23666666666666666, 0.4600000000000001, 0.87, 0.31666666666666665]\n",
      "weighted_auroc:  0.4708333333333334\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.29333333333333333, 0.44666666666666666, 0.7733333333333333, 0.38]\n",
      "weighted_auroc:  0.4733333333333333\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.24333333333333335, 0.59, 0.7633333333333333, 0.32]\n",
      "weighted_auroc:  0.4791666666666667\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.30333333333333334, 0.48333333333333334, 0.74, 0.29333333333333333]\n",
      "weighted_auroc:  0.45499999999999996\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.26666666666666666, 0.39999999999999997, 0.7433333333333333, 0.29000000000000004]\n",
      "weighted_auroc:  0.425\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.32, 0.5033333333333333, 0.8533333333333334, 0.3233333333333333]\n",
      "weighted_auroc:  0.5\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.2833333333333333, 0.5499999999999999, 0.7933333333333333, 0.35]\n",
      "weighted_auroc:  0.49416666666666664\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.30000000000000004, 0.45999999999999996, 0.7933333333333333, 0.3433333333333333]\n",
      "weighted_auroc:  0.4741666666666666\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.27666666666666667, 0.37333333333333335, 0.8233333333333333, 0.36666666666666664]\n",
      "weighted_auroc:  0.45999999999999996\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.3666666666666667, 0.47000000000000003, 0.7833333333333333, 0.41000000000000003]\n",
      "weighted_auroc:  0.5075000000000001\n",
      "weighted_F1:  nan\n"
     ]
    }
   ],
   "source": [
    "total_weighted_acc_list = []\n",
    "total_weighted_auroc_list = []\n",
    "### 模型验证\n",
    "for i in range(10):\n",
    "    test_loader_nums = len(test_data_loader.dataset)\n",
    "    test_probs = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_gt    = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_k  =0\n",
    "    models[0].eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "            test_data_batch = test_data_batch.cuda()\n",
    "            test_label_batch = test_label_batch.cuda()\n",
    "            test_outputs, _ = models[0](test_data_batch.cuda())\n",
    "            test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "            test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "            test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "            test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "            test_k += test_outputs.shape[0]\n",
    "        test_label = np.argmax(test_gt, axis=1)\n",
    "        test_pred = np.argmax(test_probs, axis=1)\n",
    "        weighted_auroc = get_weighted_auc_f1(test_probs, test_pred, test_label)\n",
    "\n",
    "        cm = confusion_matrix(test_label, test_pred)\n",
    "        dataset_list = [10, 10, 10, 10]  # , 7\n",
    "        acc_list = []\n",
    "        weighted_acc = 0.0\n",
    "        for i in range(len(dataset_list)):\n",
    "            weight = dataset_list[i] / sum(dataset_list)\n",
    "            correct = cm[i][i]\n",
    "            acc = float(correct) / dataset_list[i]\n",
    "            acc_list.append(acc)\n",
    "            weighted_acc += weight*acc \n",
    "        total_weighted_auroc_list.append(weighted_auroc)\n",
    "        total_weighted_acc_list.append(weighted_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f2187-e2de-4e7e-977e-b1d98071fa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1e828-5b21-44a0-a6a3-9fa1735da3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdf66a-dd91-4c5d-a5e6-9712e148887a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e95c4-61d9-4172-a82c-b42efd261c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c743c6d-2d2d-4581-ab2a-755b5a3fc27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "214aea68-b7fe-4562-8192-4b9332291a72",
   "metadata": {},
   "source": [
    "21s   \\\n",
    "12619 mb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
