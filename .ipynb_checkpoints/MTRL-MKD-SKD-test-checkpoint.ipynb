{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43265239-e20e-45e9-8742-05f6ba404ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/chenweilin/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('./CMR-AI/mmaction/')\n",
    "sys.path.append('./CMR-AI/')\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from swinTransformer3D_origin import SwinTransformer3D\n",
    "from mutil_class_loss import FocalLoss, cal_auc\n",
    "from weighted_auc_f1 import get_weighted_auc_f1\n",
    "from load_dataset import ACDC\n",
    "from utilsss import generate_mask_matrix, pruning_mask, row_softmax\n",
    "from Policy import Policy, train_agent\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a2cbbe-e30c-4769-b61c-03ce131a0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'train'\n",
    "TEST  = 'test'\n",
    "FINE_TUNE = 'fine_tune'\n",
    "\n",
    "phase = TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e2dcfa-ca3a-48f3-b248-3ef55cf05a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3214.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model is 3\n"
     ]
    }
   ],
   "source": [
    "## Task Split\n",
    "num_class = {\n",
    "        0: ['HCM', 'RV', 'DCM', 'MINF'],\n",
    "        1: ['HCM', 'RV'], \n",
    "        2: ['DCM', 'MINF'],\n",
    "    }\n",
    "\n",
    "save_models = {0: 'full', 1: '1', 2: '2'}\n",
    "dummy_labels = num_class[0]\n",
    "\n",
    "models = []\n",
    "for _, v in num_class.items():\n",
    "    num_class_ = len(v)\n",
    "    models.append(SwinTransformer3D(num_class=num_class_))\n",
    "models_modules = []\n",
    "for i in range(len(models)):\n",
    "    models_modules.append(models[i].modules())\n",
    "print(f'Total model is {len(models)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dc1377-6eab-4e27-8686-aae81fef7d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer anem: head.fc_cls.weight | size: torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "## RL agent\n",
    "input_size, teacher_num = 0, len(models)\n",
    "for name, param in list(models[0].named_parameters())[-2:-1]:\n",
    "    print(f\"layer anem: {name} | size: {param.size()}\")\n",
    "    input_size = param.size()[-1]\n",
    "agent = Policy(input_size=input_size, teacher_num=teacher_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f243e565-2372-4d11-98da-9a5e0a554e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train test phase: load weightd from local file pth.\n",
      "No.1 model load pretrained weighted end.\n",
      "train test phase: load weightd from local file pth.\n",
      "No.2 model load pretrained weighted end.\n",
      "train test phase: load weightd from local file pth.\n",
      "No.3 model load pretrained weighted end.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    if phase == TRAIN:\n",
    "        _pretrained_dict = torch.load(r'./organmnist3d/organmnist3d_250.pth')\n",
    "        pretrained_dict = {}\n",
    "        for k, v in _pretrained_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                new_key = k.replace('module.', '')\n",
    "            elif k.startswith('cls_head.'):\n",
    "                new_key = k.replace('cls_head.', '')\n",
    "            else:\n",
    "                new_key = k \n",
    "            pretrained_dict[new_key] = v\n",
    "    else:\n",
    "        if phase == TEST:\n",
    "            print(f'train test phase: load weightd from local file pth.')\n",
    "            _pretrained_dict = torch.load(f'./train_model/MTRL-MKD-SKD/full/epoch_200.pth')\n",
    "            pretrained_dict = {}\n",
    "            for k, v in _pretrained_dict.items():\n",
    "                if k.startswith('module.'):\n",
    "                    new_key = k.replace('module.', '')\n",
    "                elif k.startswith('cls_head.'):\n",
    "                    new_key = k.replace('cls_head.', '')\n",
    "                else:\n",
    "                    new_key = k\n",
    "\n",
    "                pretrained_dict[new_key] = v\n",
    "    model_dict = models[i].state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    models[i].load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f'No.{i+1} model load pretrained weighted end.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f094f185-3d4f-4f98-8bda-2b4ce52476da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load mask_matrix from local file.\n"
     ]
    }
   ],
   "source": [
    "if phase == TEST or phase == FINE_TUNE:\n",
    "    print(f'load mask_matrix from local file.')\n",
    "    import pickle\n",
    "    with open(f'./mask_matrix.npz', 'rb') as f:\n",
    "        mask_matrix_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c174126-8281-44e7-a37d-21ed4e6bd1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PatchMerging_module_idx = [38, 73, 348]\n",
    "\n",
    "WindowAttention3D_module_idx = [45, 47, 60, 62, 80, 82, 95, 97,110, 112,  125, 127, 140, 142, 155, 157, 170, 172, \\\n",
    "                                185, 187, 200, 202, 215, 217, 230, 232, 245, 247, 260, 262, 275, 277, 290, 292, 305, \\\n",
    "                                307, 320, 322, 335, 337, 355, 357, 370, 372, ]\n",
    "\n",
    "Mlp_module_idx = [18, 20, 33, 35, 53, 55, 68, 70, 88, 90, 103, 105, 118, 120, 133, 135, 148, 150, 163, 165, 178, 180, \\\n",
    "                  193, 195, 208, 210, 223, 225, 238, 240,253, 255,  268, 270, 283, 285, 298, 300, 313, 315, 328, 330, 343, \\\n",
    "                  345, 363, 365, 378, 380]\n",
    "                   \n",
    "if phase == TRAIN:\n",
    "    print(f'start to generate mask_matrix.')\n",
    "    mask_matrix_dict = {}\n",
    "    for module_idx, module in enumerate(models[0].modules()):\n",
    "        if module_idx in PatchMerging_module_idx or module_idx in Mlp_module_idx: continue\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if module.weight.data.numel() < 50000: continue\n",
    "            mask = torch.ByteTensor(module.weight.data.size()).fill_(0)\n",
    "            mask_matrix_dict[module_idx] = generate_mask_matrix(mask.numpy())\n",
    "\n",
    "if phase == TRAIN:\n",
    "    mask_list = []\n",
    "    for module_idx, modules in enumerate(zip(models[0].modules(), models[1].modules(), models[2].modules())):\n",
    "        if module_idx not in mask_matrix_dict.keys():  continue\n",
    "        for mask_index, module in enumerate(modules):\n",
    "            if mask_index == 0: continue\n",
    "            with torch.no_grad():\n",
    "                module.weight[mask_matrix_dict[module_idx] != mask_index] = \\\n",
    "                    module.weight[mask_matrix_dict[module_idx] != mask_index].detach().requires_grad_(False)\n",
    "                    \n",
    "if phase == TRAIN:\n",
    "    print(f'save mask_matrix to local file.')\n",
    "    import pickle\n",
    "    with open(f'./mask_matrix.npz', 'wb') as f:\n",
    "        pickle.dump(mask_matrix_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1430a57d-4068-4f7e-ac3b-3eee0fe5acee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([45, 47, 60, 62, 80, 82, 95, 97, 110, 112, 125, 127, 140, 142, 155, 157, 170, 172, 185, 187, 200, 202, 215, 217, 230, 232, 245, 247, 260, 262, 275, 277, 290, 292, 305, 307, 320, 322, 335, 337, 355, 357, 370, 372])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d021bd5-6941-42e9-8874-3fe15c84efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./roi_processed/training/train_data.csv', encoding='GBK')\n",
    "test_data = pd.read_csv('./roi_processed/testing/test_data.csv', encoding='GBK')\n",
    "\n",
    "train_data = train_data[train_data['Finding Labels'].isin(dummy_labels)]\n",
    "test_data = test_data[test_data['Finding Labels'].isin(dummy_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29be464-2ea9-4bcf-8094-d22ca218dfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    train_data[label] = train_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9357285b-ecf1-40ad-9ef5-7f7f4d490bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    test_data[label] = test_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "600a4135-e785-4364-b605-c2515eb40907",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target_vector'] = train_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])\n",
    "test_data['target_vector'] = test_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f4ecae2-6ef9-4e38-ba10-a785847dd3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size：\n",
      "HCM     20.0\n",
      "RV      20.0\n",
      "DCM     20.0\n",
      "MINF    20.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clean_labels = train_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(f'train data size：')\n",
    "print(clean_labels) # view tabular results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40b0d773-93fc-4ca7-8cf9-6f6194900603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size：\n",
      "HCM     10.0\n",
      "RV      10.0\n",
      "DCM     10.0\n",
      "MINF    10.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'test size：')\n",
    "clean_labels = test_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(clean_labels) # view tabular results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a00694-9097-40a2-8de9-0cda171b89c7",
   "metadata": {},
   "source": [
    "### model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a420aaea-5690-4458-b7cf-232de331180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.0005\n",
    "batch_size = 8\n",
    "max_epoch = 600\n",
    "\n",
    "# 将模型放到多 GPU 上\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    for i in range(len(models)):\n",
    "        models[i] = nn.DataParallel(models[i])\n",
    "    \n",
    "for i in range(len(models)):\n",
    "    models[i] = models[i].cuda()\n",
    "    \n",
    "fn_loss  = FocalLoss(device = 'cuda:0', gamma = 2.).to('cuda:0')\n",
    "kl_loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "cross_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizers = []\n",
    "for i in range(len(models)):\n",
    "    optimizers.append(torch.optim.SGD(models[i].parameters(), lr=base_lr))\n",
    "\n",
    "agent_optimizer = torch.optim.SGD(agent.parameters(), lr=base_lr)\n",
    "    \n",
    "train_acdc_data = ACDC(data=train_data, phase = 'train', img_size=(224, 224))\n",
    "train_data_loader = DataLoader(train_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "test_acdc_data = ACDC(data=test_data, phase = 'test', img_size=(224, 224))\n",
    "test_data_loader = DataLoader(test_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0abf1f39-8813-4f12-8e30-5b289fda4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    for batch_idx, (batch_data, batch_finding, batch_label) in enumerate(test_data_loader):\n",
    "        # print(batch_data.shape)\n",
    "        for i in range(batch_data.shape[0]):\n",
    "            print(f'--------- {i} -----------')\n",
    "            for j in range(batch_data.shape[2]):\n",
    "                img_ = batch_data[i, 0, j, :, :].detach().numpy()\n",
    "                plt.imshow(img_, cmap='gray')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac16c78-cf32-4804-9e16-aa638f320ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == TRAIN:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    # 初始化 TensorBoard\n",
    "    writer = SummaryWriter(log_dir='./runs/VSTMTRL-ACDC')  # 指定日志保存路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d61da6-d9c7-48f6-a49d-c20c15af2e50",
   "metadata": {},
   "source": [
    "### model eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e417e409-e953-4cf6-932e-31ced82c65f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "auc_list : [0.9366666666666666, 0.9966666666666667, 0.9633333333333333, 0.95]\n",
      "weighted_auroc:  0.9616666666666667\n",
      "weighted_F1:  0.899749373433584\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9333333333333333, 1.0, 0.9833333333333334, 0.9666666666666666]\n",
      "weighted_auroc:  0.9708333333333334\n",
      "weighted_F1:  0.899749373433584\n",
      "--------------------------------------------------\n",
      "auc_list : [0.93, 1.0, 0.9833333333333334, 0.9733333333333334]\n",
      "weighted_auroc:  0.9716666666666667\n",
      "weighted_F1:  0.899749373433584\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9766666666666666, 1.0, 0.9833333333333334, 0.9766666666666667]\n",
      "weighted_auroc:  0.9841666666666666\n",
      "weighted_F1:  0.899749373433584\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9633333333333333, 1.0, 0.98, 0.95]\n",
      "weighted_auroc:  0.9733333333333334\n",
      "weighted_F1:  0.8749373433583961\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9400000000000001, 1.0, 0.9766666666666667, 0.9266666666666667]\n",
      "weighted_auroc:  0.9608333333333333\n",
      "weighted_F1:  0.8749373433583961\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9333333333333333, 1.0, 0.98, 0.9733333333333334]\n",
      "weighted_auroc:  0.9716666666666667\n",
      "weighted_F1:  0.8749373433583961\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9866666666666667, 1.0, 0.98, 0.9600000000000001]\n",
      "weighted_auroc:  0.9816666666666667\n",
      "weighted_F1:  0.8773182957393484\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9233333333333333, 1.0, 0.98, 0.9466666666666667]\n",
      "weighted_auroc:  0.9625\n",
      "weighted_F1:  0.899749373433584\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9233333333333333, 1.0, 0.9866666666666667, 0.98]\n",
      "weighted_auroc:  0.9725\n",
      "weighted_F1:  0.924937343358396\n"
     ]
    }
   ],
   "source": [
    "total_acc_list = []\n",
    "total_auroc_list = []\n",
    "\n",
    "total_weight_auroc_list = []\n",
    "total_weight_acc_list = []\n",
    "### eval.\n",
    "for i in range(10):\n",
    "    test_loader_nums = len(test_data_loader.dataset)\n",
    "    test_probs = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_gt    = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_k  =0\n",
    "    models[0].eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "            test_data_batch = test_data_batch.cuda()\n",
    "            test_label_batch = test_label_batch.cuda()\n",
    "            test_outputs, _ = models[0](test_data_batch.cuda())\n",
    "            test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "            test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "            test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "            test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "            test_k += test_outputs.shape[0]\n",
    "        test_label = np.argmax(test_gt, axis=1)\n",
    "        test_pred = np.argmax(test_probs, axis=1)\n",
    "        weight_auc, auc_list = get_weighted_auc_f1(test_probs, test_pred, test_label)\n",
    "\n",
    "        cm = confusion_matrix(test_label, test_pred)\n",
    "        dataset_list = [10, 10, 10, 10]  # , 7\n",
    "        acc_list = []\n",
    "        weighted_acc = 0.0\n",
    "        for i in range(len(dataset_list)):\n",
    "            weight = dataset_list[i] / sum(dataset_list)\n",
    "            correct = cm[i][i]\n",
    "            acc = float(correct) / dataset_list[i]\n",
    "            acc_list.append(acc)\n",
    "            weighted_acc += weight*acc \n",
    "        \n",
    "        total_auroc_list.append(auc_list)\n",
    "        total_acc_list.append(acc_list)\n",
    "        total_weight_auroc_list.append(weight_auc)\n",
    "        total_weight_acc_list.append(weighted_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66e4c252-88aa-4d61-bb80-6ee325b96eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9616666666666667, 0.9708333333333334, 0.9716666666666667, 0.9841666666666666, 0.9733333333333334, 0.9608333333333333, 0.9716666666666667, 0.9816666666666667, 0.9625, 0.9725]\n",
      "[0.8999999999999999, 0.8999999999999999, 0.8999999999999999, 0.8999999999999999, 0.875, 0.875, 0.875, 0.875, 0.8999999999999999, 0.9249999999999999]\n"
     ]
    }
   ],
   "source": [
    "print(total_weight_auroc_list)\n",
    "print(total_weight_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "847155a2-6a51-474e-a193-4b934f31d49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "0.9446666666666668 0.021457969252574753\n",
      "0.9996666666666666 0.0009999999999999898\n",
      "0.9796666666666667 0.006046119049072383\n",
      "0.9603333333333335 0.016017351702311944\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array(total_auroc_list)\n",
    "print(auc_arr.shape)\n",
    "for i in range(auc_arr.shape[-1]):\n",
    "    auc_arr_cls = auc_arr[:, i]\n",
    "    mean = np.mean(auc_arr_cls)\n",
    "    std = np.std(auc_arr_cls)\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d84efc83-dda6-4fe5-bed0-78aaa4e818e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "0.9 0.0\n",
      "1.0 0.0\n",
      "0.8600000000000001 0.04898979485566355\n",
      "0.8099999999999999 0.029999999999999995\n"
     ]
    }
   ],
   "source": [
    "acc_arr = np.array(total_acc_list)\n",
    "print(acc_arr.shape)\n",
    "for i in range(auc_arr.shape[-1]):\n",
    "    acc_arr_cls = acc_arr[:, i]\n",
    "    mean = np.mean(acc_arr_cls)\n",
    "    std = np.std(acc_arr_cls)\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f72a5-806d-4332-8ed3-4cf05891cc88",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
