{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9398ce15-1861-4a36-aede-6a99f432f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/chenweilin/.local/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('./unetr_pp/')\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from network_architecture.acdc.unetr_pp_acdc_cls import UNETR_PP\n",
    "\n",
    "from mutil_class_loss import FocalLoss, cal_auc\n",
    "from weighted_auc_f1 import get_weighted_auc_f1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b54e5ab-6184-4b1f-8656-8d7ff6e1806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dummy_labels =  ['HCM', 'RV', 'DCM', 'MINF']\n",
    "num_class = len(dummy_labels)\n",
    "unter_pp = UNETR_PP(in_channels=1, out_channels=4, dims=[32, 64, 128, 256])\n",
    "\n",
    "phase = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2fa09d-2dd5-45ea-919b-f9c7c323d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == 'train':\n",
    "    pretrained_dict = torch.load(\"../unetr_plus_plus-main/Acdc_ckpt/model_final_checkpoint.model\", map_location=\"cpu\")\n",
    "    pretrained_dict  = pretrained_dict['state_dict'] \n",
    "    model_dict = unter_pp.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    index = 0\n",
    "    for name, param in unter_pp.named_parameters():\n",
    "        index += 1\n",
    "        if index == 1 or index == 2: continue\n",
    "\n",
    "        if name in pretrained_dict and pretrained_dict[name].size() == param.size():\n",
    "            # print(f\"Loading {name} from pretrained\")\n",
    "            param.data.copy_(pretrained_dict[name])\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Skipping {name}, not found or size mismatch\")\n",
    "else:\n",
    "    _pretrained_dict = torch.load('./train_model/UNTERPP/epoch_200.pth')\n",
    "    pretrained_dict = {}\n",
    "    for k, v in _pretrained_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_key = k.replace('module.', '')\n",
    "        elif k.startswith('cls_head.'):\n",
    "            new_key = k.replace('cls_head.', '')\n",
    "        else:\n",
    "            new_key = \n",
    "\n",
    "        pretrained_dict[new_key] = v\n",
    "    model_dict = unter_pp.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    unter_pp.load_state_dict(pretrained_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca9176-3c94-4636-a7a5-fd8644ccc20e",
   "metadata": {},
   "source": [
    "加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d346e7-3446-4111-a7ab-5e3de8b9cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./roi_processed/training/train_data.csv', encoding='GBK')\n",
    "test_data = pd.read_csv('./roi_processed/testing/test_data.csv', encoding='GBK')\n",
    "\n",
    "train_data = train_data[train_data['Finding Labels'].isin(dummy_labels)]\n",
    "test_data = test_data[test_data['Finding Labels'].isin(dummy_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4530f8-0141-4eba-84c1-6189aaf9aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    train_data[label] = train_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)\n",
    "    \n",
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    test_data[label] = test_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357d289a-735f-49c1-bc75-45d658aa6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target_vector'] = train_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])\n",
    "test_data['target_vector'] = test_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f4e479-3cbf-4d37-a4d2-d898de93eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size：\n",
      "HCM     20.0\n",
      "RV      20.0\n",
      "DCM     20.0\n",
      "MINF    20.0\n",
      "dtype: float64\n",
      "test size：\n",
      "HCM     10.0\n",
      "RV      10.0\n",
      "DCM     10.0\n",
      "MINF    10.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "clean_labels = train_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(f'train size：')\n",
    "print(clean_labels)\n",
    "\n",
    "print(f'test size：')\n",
    "clean_labels = test_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a8f353-c611-42ad-b757-b6492ec6c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/chenweilin/.local/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "import nibabel as nib\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class ACDC(torch.utils.data.Dataset):\n",
    "    def __init__(self, data=None, phase = 'train', img_size=(224, 224)):\n",
    "        self.img_size = img_size\n",
    "        self.datas = data\n",
    "     \n",
    "        self.seq = iaa.Sequential([\n",
    "            iaa.PadToFixedSize(width=160, height=160, position=\"center\"),\n",
    "            iaa.GaussianBlur(sigma=(0.1, 0.25)),\n",
    "            iaa.Affine(rotate=(-45, 45)),\n",
    "        ], random_order=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "    \n",
    "\n",
    "    def pad_or_truncate_T(self, video_array, target_frames=25):\n",
    "        C, T, H, W = video_array.shape\n",
    "\n",
    "        if T == target_frames:\n",
    "            return video_array\n",
    "\n",
    "        elif T > target_frames:\n",
    "            return video_array[:, :target_frames, :, :]\n",
    "\n",
    "        else:\n",
    "            pad_frames = target_frames - T\n",
    "            repeat_times = (pad_frames + T - 1) // T + 1\n",
    "            repeated_frames = np.tile(video_array[:, :T, :, :], (1, repeat_times, 1, 1))[:, :pad_frames + T, :, :]\n",
    "            return repeated_frames  # 3 25 224 224\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fpath = self.datas.iloc[index]['full_path']\n",
    "        linux_path = fpath.replace(\"\\\\\", \"/\")\n",
    "        image = nib.load(linux_path)\n",
    "        image_array = image.get_fdata()\n",
    "        image_array=np.transpose(image_array, (3, 2, 0, -1, 1))[0]\n",
    "        \n",
    "        seq_det = self.seq.to_deterministic()\n",
    "        image_list = seq_det(images=image_array.astype(np.float32))\n",
    "        \n",
    "        image_array = np.array(image_list)\n",
    "        image_array=np.transpose(image_array, (3, 0, 2, 1))\n",
    "        if image_array.shape[0] != 3 or image_array.shape[1] < 5 or image_array.shape[2] != 160 or image_array.shape[3] != 160:\n",
    "            print(f\"problem data : {fpath}\")\n",
    "            print(f'image_array {image_array.shape}')\n",
    "        image_array = self.pad_or_truncate_T(image_array)\n",
    "        \n",
    "        image_array = image_array[:, :16, :, :]\n",
    "        image_tensor = torch.from_numpy(image_array).float()\n",
    "        label = self.datas.iloc[index]['target_vector']\n",
    "        label = label.astype(np.int64)\n",
    "        label = torch.from_numpy(label).float()\n",
    "        \n",
    "        image_tensor = image_tensor[1].unsqueeze(0)\n",
    "        return image_tensor, self.datas.iloc[index]['Finding Labels'], label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fcffba-2f40-47c9-9c4f-f0d7a7c75fda",
   "metadata": {},
   "source": [
    "model train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560af058-b1eb-4b47-aa7b-9a8d1be0233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.0005\n",
    "batch_size = 8\n",
    "max_epoch = 600\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'device {device}')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "unter_pp.to(device)\n",
    "unter_pp = unter_pp.to(device)\n",
    "fn_loss  = FocalLoss(device = device, gamma = 2.).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(unter_pp.parameters(), lr=base_lr)\n",
    "\n",
    "train_acdc_data = ACDC(data=train_data, phase = 'train', img_size=(224, 224))\n",
    "train_data_loader = DataLoader(train_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "test_acdc_data = ACDC(data=test_data, phase = 'test', img_size=(224, 224))\n",
    "test_data_loader = DataLoader(test_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601b71c5-c6ec-4ce5-91f3-2f09e1fef462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir=f'./runs/UNTER++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125f6423-7ad6-42ff-83a2-3983d68b07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == 'train':\n",
    "    import time\n",
    "    for epoch_num in range(0, max_epoch):\n",
    "        print(f\"--------> epoch_num: {epoch_num}\")\n",
    "        train_loader_nums = len(train_data_loader.dataset)\n",
    "        probs = np.zeros((train_loader_nums, num_class), dtype = np.float32)\n",
    "        gt    = np.zeros((train_loader_nums, num_class), dtype = np.float32)\n",
    "        k=0\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        unter_pp.train()\n",
    "        for train_data_batch, batch_finding, train_labels_batch in train_data_loader:\n",
    "            train_data_batch = train_data_batch.to(device)\n",
    "            train_labels_batch = train_labels_batch.to(device)\n",
    "            outputs = unter_pp(train_data_batch)\n",
    "            outputs = outputs.reshape(outputs.shape[0], -1) \n",
    "            train_labels_batch = train_labels_batch.reshape(train_labels_batch.shape[0], -1)\n",
    "            probs[k: k + outputs.shape[0], :] = outputs.cpu().detach().numpy()\n",
    "            gt[   k: k + outputs.shape[0], :] = train_labels_batch.cpu().detach().numpy()\n",
    "            k += outputs.shape[0]\n",
    "\n",
    "            train_loss = fn_loss(outputs, train_labels_batch)\n",
    "            total_train_loss += train_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "            labels = torch.argmax(train_labels_batch, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        auc = cal_auc(gt, probs)\n",
    "        print(f\"epoch_num {epoch_num} av train loss {total_train_loss}  train auc {auc} train acc {correct/train_loader_nums}\")  \n",
    "        \n",
    "        writer.add_scalars('Training Metrics', {\n",
    "            'Loss': total_train_loss / train_loader_nums*20,\n",
    "            'Accuracy': correct / train_loader_nums,\n",
    "            'AUC': auc,\n",
    "        }, epoch_num)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"程序运行时间：{elapsed_time} 秒\")\n",
    "\n",
    "        lr_ = base_lr*(1-0.0009)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_\n",
    "\n",
    "        save_interval = 25  # int(max_epoch/6)\n",
    "        if (epoch_num + 1) % save_interval == 0:\n",
    "            save_mode_path = os.path.join(f'./train_model/UNTERPP', 'epoch_' + str(epoch_num+1) + '.pth')\n",
    "            torch.save(unter_pp.state_dict(), save_mode_path)\n",
    "            print(\"save model to {}\".format(save_mode_path))\n",
    "        test_interval = 25  # int(max_epoch/6)\n",
    "        if (epoch_num + 1) % test_interval == 0:\n",
    "            test_loader_nums = len(test_data_loader.dataset)\n",
    "            test_probs = np.zeros((test_loader_nums, num_class), dtype = np.float32)\n",
    "            test_gt    = np.zeros((test_loader_nums, num_class), dtype = np.float32)\n",
    "            test_k  =0\n",
    "            unter_pp.eval()\n",
    "            with torch.no_grad():\n",
    "                for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "                    test_data_batch = test_data_batch.to(\"cuda:0\")\n",
    "                    test_label_batch = test_label_batch.to(\"cuda:0\")\n",
    "                    test_outputs = unter_pp(test_data_batch)\n",
    "                    test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "                    test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "                    test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "                    test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "                    test_k += test_outputs.shape[0]\n",
    "                test_label = np.argmax(test_gt, axis=1)\n",
    "                test_pred = np.argmax(test_probs, axis=1)\n",
    "                print(f\"auc: {cal_auc(test_gt, test_probs)} | acc: {np.sum(test_label==test_pred)/test_loader_nums}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc3b69-b7fd-4787-a9be-b3c631456c15",
   "metadata": {},
   "source": [
    "### model eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14da1a2-6018-4b6f-8c7b-9e4ed485018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "auc_list : [0.9533333333333334, 0.8133333333333332, 0.7466666666666666, 0.54]\n",
      "weighted_auroc:  0.7633333333333333\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9666666666666667, 0.74, 0.72, 0.5766666666666668]\n",
      "weighted_auroc:  0.7508333333333334\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9633333333333334, 0.7566666666666666, 0.7, 0.6266666666666666]\n",
      "weighted_auroc:  0.7616666666666666\n",
      "weighted_F1:  0.4041478129713424\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9566666666666666, 0.84, 0.7066666666666667, 0.55]\n",
      "weighted_auroc:  0.7633333333333332\n",
      "weighted_F1:  0.4257970647773279\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9466666666666668, 0.6866666666666666, 0.7133333333333334, 0.5266666666666666]\n",
      "weighted_auroc:  0.7183333333333333\n",
      "weighted_F1:  0.3973039215686275\n",
      "--------------------------------------------------\n",
      "auc_list : [0.95, 0.8166666666666667, 0.76, 0.5733333333333334]\n",
      "weighted_auroc:  0.7749999999999999\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9566666666666667, 0.7166666666666668, 0.72, 0.5933333333333334]\n",
      "weighted_auroc:  0.7466666666666667\n",
      "weighted_F1:  nan\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9533333333333334, 0.8099999999999999, 0.7066666666666667, 0.4966666666666667]\n",
      "weighted_auroc:  0.7416666666666666\n",
      "weighted_F1:  0.48686560150375946\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9700000000000001, 0.75, 0.7100000000000001, 0.5766666666666667]\n",
      "weighted_auroc:  0.7516666666666667\n",
      "weighted_F1:  0.45403280929596723\n",
      "--------------------------------------------------\n",
      "auc_list : [0.9333333333333333, 0.7266666666666667, 0.7, 0.5466666666666666]\n",
      "weighted_auroc:  0.7266666666666668\n",
      "weighted_F1:  nan\n"
     ]
    }
   ],
   "source": [
    "total_acc_list = []\n",
    "total_auroc_list = []\n",
    "\n",
    "total_weight_auroc_list = []\n",
    "total_weight_acc_list = []\n",
    "### eval.\n",
    "for i in range(10):\n",
    "    test_loader_nums = len(test_data_loader.dataset)\n",
    "    test_probs = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_gt    = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_k  =0\n",
    "    unter_pp.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "            test_data_batch = test_data_batch.cuda()\n",
    "            test_label_batch = test_label_batch.cuda()\n",
    "            test_outputs = unter_pp(test_data_batch.cuda())\n",
    "            test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "            test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "            test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "            test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "            test_k += test_outputs.shape[0]\n",
    "        test_label = np.argmax(test_gt, axis=1)\n",
    "        test_pred = np.argmax(test_probs, axis=1)\n",
    "        weight_auc, auc_list = get_weighted_auc_f1(test_probs, test_pred, test_label)\n",
    "\n",
    "        cm = confusion_matrix(test_label, test_pred)\n",
    "        dataset_list = [10, 10, 10, 10] \n",
    "        acc_list = []\n",
    "        weighted_acc = 0.0\n",
    "        for i in range(len(dataset_list)):\n",
    "            weight = dataset_list[i] / sum(dataset_list)\n",
    "            correct = cm[i][i]\n",
    "            acc = float(correct) / dataset_list[i]\n",
    "            acc_list.append(acc)\n",
    "            weighted_acc += weight*acc \n",
    "        \n",
    "        total_auroc_list.append(auc_list)\n",
    "        total_acc_list.append(acc_list)\n",
    "        total_weight_auroc_list.append(weight_auc)\n",
    "        total_weight_acc_list.append(weighted_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b1c325-89c3-4195-9634-211adc283bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "0.9550000000000001 0.010027739304327554\n",
      "0.7656666666666666 0.048489403195154095\n",
      "0.7183333333333334 0.018929694486000907\n",
      "0.5606666666666668 0.0347626875319565\n"
     ]
    }
   ],
   "source": [
    "auc_arr = np.array(total_auroc_list)\n",
    "print(auc_arr.shape)\n",
    "for i in range(auc_arr.shape[-1]):\n",
    "    auc_arr_cls = auc_arr[:, i]\n",
    "    mean = np.mean(auc_arr_cls)\n",
    "    std = np.std(auc_arr_cls)\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12345d33-34b0-4315-9a95-60f696bbdddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n",
      "0.72 0.0871779788708135\n",
      "0.05 0.05000000000000001\n",
      "0.9000000000000001 0.06324555320336757\n",
      "0.21000000000000002 0.05385164807134503\n"
     ]
    }
   ],
   "source": [
    "acc_arr = np.array(total_acc_list)\n",
    "print(acc_arr.shape)\n",
    "for i in range(auc_arr.shape[-1]):\n",
    "    acc_arr_cls = acc_arr[:, i]\n",
    "    mean = np.mean(acc_arr_cls)\n",
    "    std = np.std(acc_arr_cls)\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6918e-60ac-4435-a9ff-5ccca8d4f48f",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
