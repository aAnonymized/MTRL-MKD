{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43265239-e20e-45e9-8742-05f6ba404ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('./CMR-AI/mmaction/')\n",
    "sys.path.append('./CMR-AI/')\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from swinTransformer3D_origin import SwinTransformer3D\n",
    "from mutil_class_loss import FocalLoss, cal_auc, get_alpha, kd_loss\n",
    "from weighted_auc_f1 import get_weighted_auc_f1\n",
    "from load_dataset import ACDC\n",
    "from utilsss import generate_mask_matrix, pruning_mask, row_softmax\n",
    "from Policy import Policy, train_agent\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import SimpleITK as sitk\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2cbbe-e30c-4769-b61c-03ce131a0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'train'\n",
    "TEST  = 'test'\n",
    "FINE_TUNE = 'fine_tune'\n",
    "\n",
    "phase = TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2dcfa-ca3a-48f3-b248-3ef55cf05a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task Split\n",
    "num_class = {\n",
    "        0: ['HCM', 'RV', 'DCM', 'MINF'],\n",
    "        1: ['HCM', 'RV'], \n",
    "        2: ['DCM', 'MINF'],\n",
    "    }\n",
    "\n",
    "save_models = {0: 'full', 1: '1', 2: '2'}\n",
    "dummy_labels = num_class[0]\n",
    "\n",
    "models = []\n",
    "for _, v in num_class.items():\n",
    "    num_class_ = len(v)\n",
    "    models.append(SwinTransformer3D(num_class=num_class_))\n",
    "models_modules = []\n",
    "for i in range(len(models)):\n",
    "    models_modules.append(models[i].modules())\n",
    "print(f'Total model is {len(models)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc1377-6eab-4e27-8686-aae81fef7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL agent\n",
    "input_size, teacher_num = 0, len(models)\n",
    "for name, param in list(models[0].named_parameters())[-2:-1]:\n",
    "    print(f\"layer anem: {name} | size: {param.size()}\")\n",
    "    input_size = param.size()[-1]\n",
    "agent = Policy(input_size=input_size, teacher_num=teacher_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243e565-2372-4d11-98da-9a5e0a554e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "    if phase == TRAIN:\n",
    "        _pretrained_dict = torch.load(r'./organmnist3d_250.pth')\n",
    "        pretrained_dict = {}\n",
    "        for k, v in _pretrained_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                new_key = k.replace('module.', '')\n",
    "            elif k.startswith('cls_head.'):\n",
    "                new_key = k.replace('cls_head.', '')\n",
    "            else:\n",
    "                new_key = k \n",
    "            pretrained_dict[new_key] = v\n",
    "    else:\n",
    "        if phase == TEST:\n",
    "            print(f'train test phase: load weightd from local file pth.')\n",
    "            _pretrained_dict = torch.load(f'./MTRL-MKD-SKD/full/best_model.pth')\n",
    "            pretrained_dict = {}\n",
    "            for k, v in _pretrained_dict.items():\n",
    "                if k.startswith('module.'):\n",
    "                    new_key = k.replace('module.', '')\n",
    "                # 移除 'cls_head.' 前缀\n",
    "                elif k.startswith('cls_head.'):\n",
    "                    new_key = k.replace('cls_head.', '')\n",
    "                else:\n",
    "                    new_key = k  # 如果没有前缀，则保持不变\n",
    "\n",
    "                pretrained_dict[new_key] = v\n",
    "    model_dict = models[i].state_dict()\n",
    "    # 过滤掉不匹配的参数\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    models[i].load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f'No.{i+1} model load pretrained weighted end.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094f185-3d4f-4f98-8bda-2b4ce52476da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == TEST or phase == FINE_TUNE:\n",
    "    print(f'load mask_matrix from local file.')\n",
    "    import pickle\n",
    "    with open(f'./mask_matrix.npz', 'rb') as f:\n",
    "        mask_matrix_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c174126-8281-44e7-a37d-21ed4e6bd1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PatchMerging_module_idx = [38, 73, 348]\n",
    "\n",
    "WindowAttention3D_module_idx = [44, 45, 47, 59, 60, 62, 79, 80, 82, 94, 95, 97, 99, 110, 112,  124, 125, 127, 139, 140, 142, 154, 155, 157, 169, 170, 172, \\\n",
    "                                184, 185, 187, 199, 200, 202, 214, 215, 217, 229, 230, 232, 244, 245, 247, 259, 260, 262, 274, 275, 277, 289, 290, 292, 304, 305, \\\n",
    "                                307, 319, 320, 321, 322, 334, 335, 337, 354, 355, 357, 369, 370, 372, ]\n",
    "\n",
    "Mlp_module_idx = [18, 20, 33, 35, 53, 55, 68, 70, 88, 90, 103, 105, 118, 120, 133, 135, 148, 150, 163, 165, 178, 180, \\\n",
    "                  193, 195, 208, 210, 223, 225, 238, 240,253, 255,  268, 270, 283, 285, 298, 300, 313, 315, 328, 330, 343, \\\n",
    "                  345, 363, 365, 378, 380]\n",
    "                   \n",
    "if phase == TRAIN:\n",
    "    print(f'start to generate mask_matrix.')\n",
    "    mask_matrix_dict = {}\n",
    "    for module_idx, module in enumerate(models[0].modules()):\n",
    "        if module_idx in PatchMerging_module_idx or module_idx in Mlp_module_idx: continue\n",
    "        if hasattr(module, \"qkv\"):\n",
    "            if isinstance(module.qkv, nn.Linear):\n",
    "                if module.qkv.weight.data.numel() < 50000: continue\n",
    "                mask = torch.ByteTensor(module.qkv.weight.data.size()).fill_(0)\n",
    "                mask_matrix_dict[module_idx] = generate_mask_matrix(mask.numpy())\n",
    "        else:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if module.weight.data.numel() < 50000: continue\n",
    "                mask = torch.ByteTensor(module.weight.data.size()).fill_(0)\n",
    "                mask_matrix_dict[module_idx] = generate_mask_matrix(mask.numpy())\n",
    "\n",
    "if phase == TRAIN:\n",
    "    mask_list = []\n",
    "    for module_idx, modules in enumerate(zip(models[0].modules(), models[1].modules(), models[2].modules())):\n",
    "        if module_idx not in mask_matrix_dict.keys():  continue\n",
    "        for mask_index, module in enumerate(modules):\n",
    "            if hasattr(module, \"qkv\"):\n",
    "                if mask_index == 0: \n",
    "                    pass\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        module.qkv.weight[mask_matrix_dict[module_idx] != mask_index] = \\\n",
    "                            module.qkv.weight[mask_matrix_dict[module_idx] != mask_index].detach().requires_grad_(False)\n",
    "            else:\n",
    "                if mask_index == 0: \n",
    "                    pass\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        module.weight[mask_matrix_dict[module_idx] != mask_index] = \\\n",
    "                            module.weight[mask_matrix_dict[module_idx] != mask_index].detach().requires_grad_(False)\n",
    "                    \n",
    "if phase == TRAIN:\n",
    "    print(f'save mask_matrix to local file.')\n",
    "    import pickle\n",
    "    with open(f'./mask_matrix.npz', 'wb') as f:\n",
    "        pickle.dump(mask_matrix_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430a57d-4068-4f7e-ac3b-3eee0fe5acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d021bd5-6941-42e9-8874-3fe15c84efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./sax_roi_processed/training/train_data.csv', encoding='GBK')\n",
    "test_data = pd.read_csv('./sax_roi_processed/testing/test_data.csv', encoding='GBK')\n",
    "\n",
    "train_data = train_data[train_data['Finding Labels'].isin(dummy_labels)]\n",
    "test_data = test_data[test_data['Finding Labels'].isin(dummy_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29be464-2ea9-4bcf-8094-d22ca218dfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    train_data[label] = train_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357285b-ecf1-40ad-9ef5-7f7f4d490bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding of Finding Labels to dummy_labels\n",
    "for label in dummy_labels:\n",
    "    test_data[label] = test_data['Finding Labels'].map(lambda result: 1.0 if label in result else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a4135-e785-4364-b605-c2515eb40907",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target_vector'] = train_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])\n",
    "test_data['target_vector'] = test_data.apply(lambda target: [target[dummy_labels].values], 1).map(lambda target: target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ecae2-6ef9-4e38-ba10-a785847dd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_labels = train_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(f'train data size：')\n",
    "print(clean_labels) # view tabular results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0d773-93fc-4ca7-8cf9-6f6194900603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test size：')\n",
    "clean_labels = test_data[dummy_labels].sum().sort_values(ascending= False) # get sorted value_count for clean labels\n",
    "print(clean_labels) # view tabular results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2cb00-9ae0-456a-a998-9e73a0271f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "for label_ in dummy_labels:\n",
    "    dataset_list.append(clean_labels[label_])\n",
    "dataset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a00694-9097-40a2-8de9-0cda171b89c7",
   "metadata": {},
   "source": [
    "## 训练开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420aaea-5690-4458-b7cf-232de331180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = \n",
    "batch_size = \n",
    "max_epoch = \n",
    "momentum = \n",
    "T = \n",
    "\n",
    "# 将模型放到多 GPU 上\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    for i in range(len(models)):\n",
    "        models[i] = nn.DataParallel(models[i])\n",
    "    \n",
    "for i in range(len(models)):\n",
    "    models[i] = models[i].cuda()\n",
    "    \n",
    "fn_loss  = FocalLoss(device = 'cuda:0', gamma = 2.).to('cuda:0')\n",
    "kl_loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "cross_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizers = []\n",
    "for i in range(len(models)):\n",
    "    optimizers.append(torch.optim.SGD(models[i].parameters(), lr=base_lr))\n",
    "\n",
    "agent_optimizer = torch.optim.SGD(agent.parameters(), lr=base_lr)\n",
    "    \n",
    "train_acdc_data = ACDC(data=train_data, phase = 'train', img_size=(224, 224))\n",
    "train_data_loader = DataLoader(train_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "test_acdc_data = ACDC(data=test_data, phase = 'test', img_size=(224, 224))\n",
    "test_data_loader = DataLoader(test_acdc_data, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf1f39-8813-4f12-8e30-5b289fda4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    for batch_idx, (batch_data, batch_finding, batch_label) in enumerate(test_data_loader):\n",
    "        # print(batch_data.shape)\n",
    "        for i in range(batch_data.shape[0]):\n",
    "            print(f'--------- {i} -----------')\n",
    "            for j in range(batch_data.shape[2]):\n",
    "                img_ = batch_data[i, 0, j, :, :].detach().numpy()\n",
    "                plt.imshow(img_, cmap='gray')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac16c78-cf32-4804-9e16-aa638f320ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == TRAIN:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    # 初始化 TensorBoard\n",
    "    writer = SummaryWriter(log_dir='./runs/VSTMTRL-ACDC')  # 指定日志保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14b1ca-a396-4cb9-ab02-ccedcc23ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase == TRAIN:\n",
    "    import time\n",
    "    pre_mutil_teacher_acc = []\n",
    "    now_mutil_teacher_acc = []\n",
    "    for i in range(1, len(models)):\n",
    "        pre_mutil_teacher_acc.append(0)\n",
    "        now_mutil_teacher_acc.append(0)\n",
    "    logit_actions_list = []\n",
    "    agent_rewars_list  = []\n",
    "    student_infos_list = []    \n",
    "    mask_list_dict= {}\n",
    "    bast_acc = 0.0\n",
    "    for epoch_num in range(0, max_epoch):\n",
    "        alpha = get_alpha(epoch_num, max_epoch)\n",
    "        # alpha = 0.7\n",
    "        print(f\"--------> epoch_num: {epoch_num}\")\n",
    "        train_loader_nums = len(train_data_loader.dataset)\n",
    "        probs = np.zeros((train_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "        gt    = np.zeros((train_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "        k_index = 0\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        for i in range(len(models)): models[i].train()\n",
    "        mutil_teacher_correct = []\n",
    "        mutil_teacher_num = []\n",
    "        for i in range(1, len(models)):\n",
    "            mutil_teacher_correct.append(0)\n",
    "            mutil_teacher_num.append(0)\n",
    "        for batch_idx, (batch_data, batch_finding, batch_label) in enumerate(train_data_loader):\n",
    "            weighted_list = []\n",
    "            mutil_teacher_label = torch.zeros_like(batch_label)\n",
    "            student_output = torch.zeros_like(batch_label)\n",
    "            pre_lables = 0\n",
    "            s_train_loss = torch.tensor(0.0, device='cuda')\n",
    "            for i, k in enumerate(num_class.keys()):\n",
    "                if i == 0: \n",
    "                    continue\n",
    "                else:\n",
    "                    ## train teacher.\n",
    "                    teacher_train_data_index = pd.Series(batch_finding).isin(num_class[k])\n",
    "                    teacher_train_data_index = teacher_train_data_index.to_numpy()\n",
    "                    weighted_list.append(np.sum(teacher_train_data_index > 0))\n",
    "                    mutil_teacher_num[i-1] += weighted_list[i-1]\n",
    "                    t_train_data = batch_data[teacher_train_data_index]\n",
    "                    t_train_label = batch_label[teacher_train_data_index][:, pre_lables:pre_lables+len(num_class[k])]\n",
    "                    if np.sum(teacher_train_data_index > 0) == 0: \n",
    "                        pre_lables += len(num_class[i])\n",
    "                        continue\n",
    "                    t_output, _ = models[i](t_train_data.cuda())\n",
    "                    \n",
    "                    mutil_teacher_label[teacher_train_data_index, pre_lables:pre_lables+len(num_class[k])] = \\\n",
    "                                            row_softmax(t_output.cpu().detach())\n",
    "                    t_output = t_output.reshape(t_output.shape[0], -1)\n",
    "                    t_train_label = t_train_label.reshape(t_train_label.shape[0], -1).cuda()\n",
    "                    t_train_loss = fn_loss(t_output, t_train_label)\n",
    "                    optimizers[i].zero_grad()\n",
    "                    t_train_loss.backward()\n",
    "                    optimizers[i].step()\n",
    "                    predicted_ = torch.argmax(t_output, 1)\n",
    "                    labels_ = torch.argmax(t_train_label.cuda(), 1)\n",
    "                    correct_ = (predicted_ == labels_).sum().item() \n",
    "                    mutil_teacher_correct[i-1] += correct_\n",
    "                    \n",
    "                    ## train student\n",
    "                    s_train_data, s_train_label = t_train_data = batch_data[teacher_train_data_index], batch_label[teacher_train_data_index]\n",
    "                    s_output, _ = models[0](s_train_data.cuda())\n",
    "                    s_output = s_output.reshape(s_output.shape[0], -1)\n",
    "                    student_output[teacher_train_data_index] = s_output.cpu()  \n",
    "                    \n",
    "                    log_s_output = torch.nn.LogSoftmax(dim=1)(student_output)\n",
    "                    s_train_loss += kd_loss(s_output.cuda(), mutil_teacher_label[teacher_train_data_index].cuda(), batch_label[teacher_train_data_index].cuda(), T, alpha)\n",
    "                    pre_lables += len(num_class[k])\n",
    "                    \n",
    "            optimizers[0].zero_grad()\n",
    "            s_train_loss.backward()\n",
    "            optimizers[0].step()\n",
    "            total_train_loss += s_train_loss\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs, student_info = models[0](batch_data.cuda())\n",
    "                predicted = torch.argmax(outputs, 1)\n",
    "                labels = torch.argmax(batch_label.cuda(), 1)\n",
    "                correct += (predicted == labels).sum().item()  \n",
    "\n",
    "            ## RL\n",
    "            if epoch_num != 0 and (batch_idx+1) % (len(train_data_loader.dataset)//batch_size//3) == 0:\n",
    "                student_infos_list.append(student_info.detach().cpu())\n",
    "                with torch.no_grad():\n",
    "                    logit_action = agent(student_info.detach().cpu())\n",
    "                if epoch_num == 0:\n",
    "                    logit_action = torch.ones_like(logit_action).cuda()\n",
    "                logit_actions_list.append(logit_action.detach().cpu())                \n",
    "\n",
    "                agent_reward = -(F.cross_entropy(outputs.cuda() ,batch_label.cuda(), reduction='none'))\n",
    "                rewards_mean = agent_reward.mean() \n",
    "                rewards_std = agent_reward.std() \n",
    "                normalized_reward = (agent_reward - rewards_mean) / rewards_std \n",
    "                normalized_reward = normalized_reward.detach().cpu()\n",
    "                normalized_reward = torch.clamp(normalized_reward, min=0, max=1)\n",
    "                agent_rewars_list.append(normalized_reward)\n",
    "\n",
    "                mean_logit_action = torch.mean(logit_action, dim=0, keepdim=True)  # shape=(1, 4)\n",
    "                print(f'RMD weights : {mean_logit_action}')\n",
    "\n",
    "                for module_index, modules in enumerate(zip(models[0].modules(), models[1].modules(), models[2].modules())):\n",
    "                    mask_list = []\n",
    "                    teacher_class_num_sum = 0\n",
    "                    if module_index not in mask_matrix_dict.keys():  continue\n",
    "                    if isinstance(modules[0], nn.Linear):\n",
    "                        if modules[0].weight.data.numel() < 50000: continue\n",
    "                        for mask_index, module in enumerate(modules):\n",
    "                            if mask_index == 0: \n",
    "                                mask_list.append([])\n",
    "                                continue  \n",
    "                            if hasattr(modules[0], \"qkv\"):\n",
    "                                weights = module.qkv.weight.data\n",
    "                            else:\n",
    "                                weights = module.weight.data\n",
    "                            rl_weights = mean_logit_action[0][mask_index-1]\n",
    "                            mask = pruning_mask(weights.cpu(), mask_matrix_dict[module_index], mask_index, k=rl_weights.cpu().numpy())  ## 1 表示 N0-PI\n",
    "                            mask_list.append(mask.cuda())\n",
    "                        all_weights_mask = torch.ones_like(modules[0].weight.data)\n",
    "                        for i in range(1, len(models)):\n",
    "                            unique_weights_mask_ = mask_list[i]\n",
    "                            unique_weights_mask = (unique_weights_mask_ >= 1).int()\n",
    "                            if hasattr(modules[0], \"qkv\"):\n",
    "                                modules[0].qkv.weight.grad.data[unique_weights_mask].fill_(0)  ## 不让更新\n",
    "                                unique_weights = unique_weights_mask * modules[i].qkv.weight.data \n",
    "                                modules[0].qkv.weight.data = (modules[0].qkv.weight.data)*(all_weights_mask-unique_weights_mask) + \\\n",
    "                                                         + momentum*(unique_weights_mask * modules[0].qkv.weight.data) + (1-momentum)*unique_weights    \n",
    "                            else:\n",
    "                                modules[0].weight.grad.data[unique_weights_mask].fill_(0)  ## 不让更新\n",
    "                                unique_weights = unique_weights_mask * modules[i].weight.data \n",
    "                                modules[0].weight.data = (modules[0].weight.data)*(all_weights_mask-unique_weights_mask) + \\\n",
    "                                                         + momentum*(unique_weights_mask * modules[0].weight.data) + (1-momentum)*unique_weights                            \n",
    "            if batch_idx != 0 and batch_idx % (len(train_data_loader)-1) == 0:  \n",
    "                train_agent(epoch=epoch_num, agent=agent, student_infos=student_infos_list, agent_rewards=agent_rewars_list, logits_agent_actions=logit_actions_list, agent_optimizer=agent_optimizer)\n",
    "                logit_actions_list.clear()\n",
    "                agent_rewars_list.clear()\n",
    "                student_infos_list.clear()\n",
    "    \n",
    "        logit_actions_list.clear()\n",
    "        agent_rewars_list.clear()\n",
    "        student_infos_list.clear()\n",
    "                 \n",
    "        for i in range(len(now_mutil_teacher_acc)):\n",
    "            now_mutil_teacher_acc[i] = mutil_teacher_correct[i]/mutil_teacher_num[i]\n",
    "      \n",
    "        print(f'mutil-teacher acc is : {now_mutil_teacher_acc}')\n",
    "        print(f\"epoch_num {epoch_num} train loss {total_train_loss} \")  \n",
    "\n",
    "        writer.add_scalars('Training Metrics', {\n",
    "            'Loss': total_train_loss,\n",
    "            'Accuracy': correct / train_loader_nums,\n",
    "        }, epoch_num)\n",
    "        \n",
    "        lr_ = base_lr*(1-0.0009)\n",
    "        T = T*(1-0.0009)\n",
    "        for i in range(len(optimizers)):\n",
    "            for param_group in optimizers[i].param_groups:\n",
    "                param_group['lr'] = lr_\n",
    "        \n",
    "        mutil_teacher_compare_result = (np.array(now_mutil_teacher_acc) > np.array(pre_mutil_teacher_acc))\n",
    "        for i in range(mutil_teacher_compare_result.shape[0]):\n",
    "            if mutil_teacher_compare_result[i]: \n",
    "                pre_mutil_teacher_acc[i] = now_mutil_teacher_acc[i]\n",
    "\n",
    "        for i in range(1, len(models)):\n",
    "            if mutil_teacher_compare_result[i-1] == False:\n",
    "                print(f'teacher model {i} dont surpressed pre batch. dont update agein.')\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"程序运行时间：{elapsed_time} 秒\")\n",
    "        test_interval = 1\n",
    "        if (epoch_num + 1) % test_interval == 0:\n",
    "            test_loader_nums = len(test_data_loader.dataset)\n",
    "            test_probs = np.zeros((test_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "            test_gt    = np.zeros((test_loader_nums, len(num_class[0])), dtype = np.float32)\n",
    "            test_k  = 0\n",
    "            models[0].eval()\n",
    "            with torch.no_grad():\n",
    "                for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "                    test_data_batch = test_data_batch.cuda()\n",
    "                    test_label_batch = test_label_batch.cuda()\n",
    "                    test_outputs, _ = models[0](test_data_batch)\n",
    "                    test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "                    test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "                    # storing model predictions for metric evaluation \n",
    "                    test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "                    test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "                    test_k += test_outputs.shape[0]\n",
    "                test_label = np.argmax(test_gt, axis=1)\n",
    "                test_pred = np.argmax(test_probs, axis=1)\n",
    "                print(f\"auc: {cal_auc(test_gt, test_probs)} | acc: {np.sum(test_label==test_pred)/test_k}\")\n",
    "                if (np.sum(test_label==test_pred)/test_k) >= bast_acc:\n",
    "                    bast_acc = (np.sum(test_label==test_pred)/test_k)\n",
    "                    for i in range(len(models)):\n",
    "                        os.makedirs(f'./train_model/MTRL-MKD-SKD/', exist_ok=True)\n",
    "                        os.makedirs(f'./train_model/MTRL-MKD-SKD/{save_models[i]}', exist_ok=True)\n",
    "                        save_mode_path = os.path.join(f'./train_model/MTRL-MKD-SKD/{save_models[i]}', 'best_model.pth')\n",
    "                        torch.save(models[i].state_dict(), save_mode_path)\n",
    "                        print(\"save model to {}\".format(save_mode_path))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40163aca-912e-4b46-b2a7-a273723b85c3",
   "metadata": {},
   "source": [
    "3 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d61da6-d9c7-48f6-a49d-c20c15af2e50",
   "metadata": {},
   "source": [
    "time 17.5   \\\n",
    "11301MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417e409-e953-4cf6-932e-31ced82c65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_list = []\n",
    "total_auroc_list = []\n",
    "\n",
    "total_weight_auroc_list = []\n",
    "total_weight_acc_list = []\n",
    "### eval.\n",
    "for i in range(10):\n",
    "    test_loader_nums = len(test_data_loader.dataset)\n",
    "    test_probs = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_gt    = np.zeros((test_loader_nums, len(dummy_labels)), dtype = np.float32)\n",
    "    test_k  =0\n",
    "    models[0].eval()\n",
    "    with torch.no_grad():\n",
    "        for test_data_batch, _, test_label_batch in test_data_loader:\n",
    "            test_data_batch = test_data_batch.cuda()\n",
    "            test_label_batch = test_label_batch.cuda()\n",
    "            test_outputs, _ = models[0](test_data_batch.cuda())\n",
    "            test_outputs = test_outputs.reshape(test_outputs.shape[0], -1)           \n",
    "            test_label_batch = test_label_batch.reshape(test_outputs.shape[0], -1)\n",
    "            test_probs[test_k: test_k + test_outputs.shape[0], :] = test_outputs.cpu().detach().numpy()\n",
    "            test_gt[   test_k: test_k + test_outputs.shape[0], :] = test_label_batch.cpu().detach().numpy()\n",
    "            test_k += test_outputs.shape[0]\n",
    "        test_label = np.argmax(test_gt, axis=1)\n",
    "        test_pred = np.argmax(test_probs, axis=1)\n",
    "        weight_auc, auc_list = get_weighted_auc_f1(test_probs, test_pred, test_label)\n",
    "\n",
    "        cm = confusion_matrix(test_label, test_pred)\n",
    "        dataset_list = [10, 10, 10, 10]  # , 7\n",
    "        acc_list = []\n",
    "        weighted_acc = 0.0\n",
    "        for i in range(len(dataset_list)):\n",
    "            weight = dataset_list[i] / sum(dataset_list)\n",
    "            correct = cm[i][i]\n",
    "            acc = float(correct) / dataset_list[i]\n",
    "            acc_list.append(acc)\n",
    "            weighted_acc += weight*acc \n",
    "        \n",
    "        total_auroc_list.append(auc_list)\n",
    "        total_acc_list.append(acc_list)\n",
    "        total_weight_auroc_list.append(weight_auc)\n",
    "        total_weight_acc_list.append(weighted_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4c252-88aa-4d61-bb80-6ee325b96eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_weight_auroc_list)\n",
    "print(total_weight_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847155a2-6a51-474e-a193-4b934f31d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_arr = np.array(total_auroc_list)\n",
    "print(auc_arr.shape)\n",
    "for i in range(auc_arr.shape[-1]):\n",
    "    auc_arr_cls = auc_arr[:, i]\n",
    "    mean = np.mean(auc_arr_cls)\n",
    "    std = np.std(auc_arr_cls)\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84efc83-dda6-4fe5-bed0-78aaa4e818e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr = np.array(total_acc_list)\n",
    "print(acc_arr.shape)\n",
    "for i in range(auc_arr.shape[-1]):\n",
    "    acc_arr_cls = acc_arr[:, i]\n",
    "    mean = np.mean(acc_arr_cls)\n",
    "    std = np.std(acc_arr_cls)\n",
    "    print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82986fb2-76bd-4519-9f05-2e694badea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(total_weight_acc_list), np.std(total_weight_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfb8de-e740-4818-81cf-30f9112b4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(total_weight_auroc_list), np.std(total_weight_auroc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f72a5-806d-4332-8ed3-4cf05891cc88",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
